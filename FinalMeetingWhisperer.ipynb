{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOrAXMthjbyG",
        "outputId": "6445aa0d-e11c-478d-e29d-4d975be30800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/800.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.3/286.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit pyngrok torch openai-whisper pydub anthropic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import whisper\n",
        "import torch\n",
        "import tempfile\n",
        "import anthropic\n",
        "import pandas as pd\n",
        "import re\n",
        "import requests\n",
        "import io\n",
        "from requests.auth import HTTPBasicAuth\n",
        "\n",
        "# === Embed your Claude API Key here ===\n",
        "claude_api_key = \"\"\n",
        "\n",
        "# UI Config\n",
        "st.set_page_config(page_title=\"📝 Meeting Whisperer\", layout=\"centered\")\n",
        "st.title(\"Meeting Whisperer with Claude\")\n",
        "\n",
        "if not claude_api_key:\n",
        "    st.warning(\"🔑 Claude API key not set.\")\n",
        "    st.stop()\n",
        "\n",
        "client = anthropic.Anthropic(api_key=claude_api_key)\n",
        "\n",
        "# Initialize session state\n",
        "for key in [\"transcript\", \"summary\", \"compliance\", \"task_matrix\"]:\n",
        "    if key not in st.session_state:\n",
        "        st.session_state[key] = None\n",
        "\n",
        "# Upload audio\n",
        "uploaded_file = st.file_uploader(\"Upload your meeting audio\", type=[\"mp3\", \"wav\", \"m4a\"])\n",
        "\n",
        "if uploaded_file and st.session_state.transcript is None:\n",
        "    with tempfile.NamedTemporaryFile(delete=False) as tmp:\n",
        "        tmp.write(uploaded_file.read())\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = whisper.load_model(\"base\").to(device)\n",
        "\n",
        "    st.info(\"🔊 Transcribing audio...\")\n",
        "    result = model.transcribe(tmp_path, fp16=torch.cuda.is_available())\n",
        "    st.session_state.transcript = result[\"text\"]\n",
        "\n",
        "transcript = st.session_state.transcript\n",
        "\n",
        "if transcript:\n",
        "    st.subheader(\"📄 Transcript\")\n",
        "    st.write(transcript)\n",
        "\n",
        "    # === Claude Summary ===\n",
        "    if st.session_state.summary is None:\n",
        "        summary_prompt = f\"\"\"\n",
        "You are a helpful assistant. Summarize the following meeting transcript and extract any action items or key decisions.\n",
        "\n",
        "Transcript:\n",
        "{transcript[:4000]}\n",
        "\"\"\"\n",
        "        with st.spinner(\"🤖 Summarizing with Claude...\"):\n",
        "            response = client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=800,\n",
        "                messages=[{\"role\": \"user\", \"content\": summary_prompt}],\n",
        "            )\n",
        "            st.session_state.summary = response.content[0].text\n",
        "\n",
        "    st.subheader(\"🧠 Summary & Action Items\")\n",
        "    st.write(st.session_state.summary)\n",
        "    st.download_button(\"📄 Download Summary\", st.session_state.summary, file_name=\"meeting_summary.txt\")\n",
        "\n",
        "    # === Task Matrix Extraction ===\n",
        "    if st.session_state.task_matrix is None:\n",
        "        task_prompt = f\"\"\"\n",
        "Extract all actionable tasks discussed in this transcript and return a Markdown table in this format:\n",
        "\n",
        "| Task | Assigned To | Deadline |\n",
        "\n",
        "Use 'Unassigned' and 'Not specified' if needed.\n",
        "\n",
        "Transcript:\n",
        "{transcript[:4000]}\n",
        "\"\"\"\n",
        "        with st.spinner(\"📋 Extracting Task Matrix...\"):\n",
        "            task_resp = client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=800,\n",
        "                messages=[{\"role\": \"user\", \"content\": task_prompt}],\n",
        "            )\n",
        "            st.session_state.task_matrix = task_resp.content[0].text\n",
        "\n",
        "    st.subheader(\"✅ Task Matrix\")\n",
        "    st.code(st.session_state.task_matrix)\n",
        "\n",
        "    # ✅ FIX: Parse Markdown Table (remove separator --- row)\n",
        "    task_df = pd.DataFrame()\n",
        "    table_match = re.search(r\"(?P<table>(\\|.+?\\|[\\r\\n]+)+)\", st.session_state.task_matrix)\n",
        "    if table_match:\n",
        "        table_text = table_match.group(\"table\")\n",
        "\n",
        "        # Remove separator row like | --- | --- | --- |\n",
        "        lines = table_text.strip().splitlines()\n",
        "        cleaned_lines = [line for line in lines if not re.match(r\"^\\|\\s*-+\\s*\\|\", line)]\n",
        "        cleaned_table = \"\\n\".join(cleaned_lines)\n",
        "\n",
        "        try:\n",
        "            task_df = pd.read_csv(io.StringIO(cleaned_table), sep=\"|\", engine=\"python\")\n",
        "            task_df = task_df.dropna(axis=1, how=\"all\")\n",
        "            task_df.columns = [c.strip() for c in task_df.columns]\n",
        "            st.dataframe(task_df)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"⚠️ Could not format task matrix: {e}\")\n",
        "    else:\n",
        "        st.warning(\"⚠️ Could not extract Markdown table from Claude response.\")\n",
        "\n",
        "    # === Jira Integration ===\n",
        "    def create_jira_ticket(base_url, project_key, email, api_token, summary, description_text):\n",
        "        url = f\"{base_url}/rest/api/3/issue\"\n",
        "        headers = {\n",
        "            \"Accept\": \"application/json\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        auth = HTTPBasicAuth(email, api_token)\n",
        "\n",
        "        description_doc = {\n",
        "            \"type\": \"doc\",\n",
        "            \"version\": 1,\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"paragraph\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": description_text}\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        payload = {\n",
        "            \"fields\": {\n",
        "                \"project\": {\"key\": project_key},\n",
        "                \"summary\": summary,\n",
        "                \"description\": description_doc,\n",
        "                \"issuetype\": {\"name\": \"Task\"}\n",
        "            }\n",
        "        }\n",
        "\n",
        "        response = requests.post(url, headers=headers, auth=auth, json=payload)\n",
        "        return response.status_code, response.json()\n",
        "\n",
        "    with st.expander(\"🪄 Create Jira Tickets from Tasks\"):\n",
        "        jira_domain = st.text_input(\"Your Jira Domain (e.g. yourteam.atlassian.net)\")\n",
        "        jira_project = st.text_input(\"Jira Project Key (e.g. ENG)\")\n",
        "        jira_email = st.text_input(\"Your Jira Email\", type=\"password\")\n",
        "        jira_token = st.text_input(\"Jira API Token\", type=\"password\")\n",
        "\n",
        "        if st.button(\"🚀 Create Jira Tickets\"):\n",
        "            if not all([jira_domain, jira_project, jira_email, jira_token]):\n",
        "                st.warning(\"Please fill in all Jira credentials.\")\n",
        "            elif task_df.empty:\n",
        "                st.warning(\"No tasks available to send.\")\n",
        "            else:\n",
        "                for _, row in task_df.iterrows():\n",
        "                    title = row[\"Task\"]\n",
        "                    assignee = row[\"Assigned To\"]\n",
        "                    deadline = row[\"Deadline\"]\n",
        "                    description_text = (\n",
        "                        f\"Auto-generated from Meeting Whisperer\\n\\n\"\n",
        "                        f\"Assigned To: {assignee}\\nDeadline: {deadline}\\n\\n\"\n",
        "                        f\"Transcript Summary:\\n{st.session_state.summary}\"\n",
        "                    )\n",
        "\n",
        "                    status, result = create_jira_ticket(\n",
        "                        base_url=f\"https://{jira_domain}\",\n",
        "                        project_key=jira_project,\n",
        "                        email=jira_email,\n",
        "                        api_token=jira_token,\n",
        "                        summary=title,\n",
        "                        description_text=description_text\n",
        "                    )\n",
        "\n",
        "                    if status == 201:\n",
        "                        st.success(f\"✅ Created: {title}\")\n",
        "                    else:\n",
        "                        st.error(f\"❌ Failed: {title} — {result}\")\n",
        "\n",
        "    # === Compliance & Sentiment ===\n",
        "    if st.session_state.compliance is None:\n",
        "        compliance_prompt = f\"\"\"\n",
        "Analyze the following transcript for compliance and sentiment indicators. Return your response in three sections:\n",
        "\n",
        "1. Sensitive Content: Identify any legally, financially, or ethically sensitive decisions or discussions.\n",
        "2. Emotionally Charged Language: Highlight statements with strong negative or positive emotions.\n",
        "3. Commitments Made: List any commitments made by name (e.g., \"Alice will send the report by Friday\").\n",
        "\n",
        "Transcript:\n",
        "{transcript[:4000]}\n",
        "\"\"\"\n",
        "        with st.spinner(\"🔍 Analyzing Compliance & Sentiment...\"):\n",
        "            compliance_response = client.messages.create(\n",
        "                model=\"claude-3-haiku-20240307\",\n",
        "                max_tokens=800,\n",
        "                messages=[{\"role\": \"user\", \"content\": compliance_prompt}],\n",
        "            )\n",
        "            st.session_state.compliance = compliance_response.content[0].text\n",
        "\n",
        "    st.subheader(\"🧑‍⚖️ Compliance & Sentiment Tags\")\n",
        "    st.markdown(st.session_state.compliance)\n",
        "\n",
        "    # === Ask a Question Section ===\n",
        "    with st.expander(\"💬 Ask a Question about the Meeting\"):\n",
        "        user_question = st.text_input(\"Ask Claude anything about this meeting:\")\n",
        "        if st.button(\"Submit Question\"):\n",
        "            if user_question.strip():\n",
        "                question_prompt = f\"\"\"\n",
        "Here is a meeting transcript:\n",
        "{transcript[:4000]}\n",
        "\n",
        "Now answer the following question about the meeting:\n",
        "{user_question}\n",
        "\"\"\"\n",
        "                with st.spinner(\"💬 Claude is answering...\"):\n",
        "                    answer = client.messages.create(\n",
        "                        model=\"claude-3-haiku-20240307\",\n",
        "                        max_tokens=500,\n",
        "                        messages=[{\"role\": \"user\", \"content\": question_prompt}],\n",
        "                    )\n",
        "                    st.success(\"✅ Answer:\")\n",
        "                    st.write(answer.content[0].text)\n",
        "            else:\n",
        "                st.warning(\"Please enter a question first.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDDNgNMpjcgK",
        "outputId": "12957d80-15f7-495b-de6f-2df0cb6f12da"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.kill()\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "# Start Streamlit in background\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "\n",
        "# Now create the tunnel using HTTP\n",
        "public_url = ngrok.connect(\"8501\", \"http\")  # explicitly specify HTTP type\n",
        "print(\"🌐 Streamlit Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Rl4JK9kGA9",
        "outputId": "927b6237-80b0-4b8c-de57-3f9ac6f47380"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌐 Streamlit Public URL: NgrokTunnel: \"https://cdec-34-135-101-167.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYd_pEgfkLp8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}